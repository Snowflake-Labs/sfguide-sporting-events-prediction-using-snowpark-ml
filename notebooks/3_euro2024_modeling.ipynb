{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2f221c-19c2-45b7-9a9b-2f593ffc490f",
   "metadata": {
    "name": "_Model_Training",
    "collapsed": false
   },
   "source": "## Model Training and Evaluation\n---\n- It's time to delve into the ML side! \n- Let's explore how we can train a model using Snowpark ML\n\n---\n\nðŸ›‘ **But hold on!** ðŸ›‘\n\nBefore we get started, make sure you have the following packages adding from the `Packages` drop down:\n- `snowflake-ml-python == 1.5.0`\n- `fastparquet == 2023.8.0`\n\n"
  },
  {
   "cell_type": "code",
   "id": "5e670b31-0dba-4d9a-89cd-d58bf542679e",
   "metadata": {
    "language": "python",
    "name": "Import_Libs",
    "collapsed": false
   },
   "outputs": [],
   "source": "import snowflake.snowpark\nfrom snowflake.snowpark.session import Session\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark import functions as F   \nfrom snowflake.snowpark.functions import udf, udtf\nfrom snowflake.snowpark.types import IntegerType, FloatType, StringType, StructField, StructType, DateType\n    \nimport pandas as pd\nimport numpy as np\nimport streamlit as st\n\nimport warnings\nwarnings.filterwarnings('ignore')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98a6d186-f20e-4939-af53-651379fa73cd",
   "metadata": {
    "language": "python",
    "name": "Get_Active_Session",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# add version tracking\napp_tag = {\n    \"origin\": \"sf_sit\",\n    \"name\": \"hol_sport_predict\",\n    \"version\": '{major: 1, minor: 0}'\n}\n\nsession.query_tag = app_tag",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eafaaaed-a002-4cc7-a23b-936292c1df27",
   "metadata": {
    "language": "python",
    "name": "Get_User_Name",
    "collapsed": false
   },
   "outputs": [],
   "source": "user_name = session.sql('select current_user()').collect()[0][0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10f33abb-d7b1-4c0f-b45a-dc5fbb45dbda",
   "metadata": {
    "language": "python",
    "name": "Function_MLOPS_Versioning",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# FUNCTION used to iterate the model version so we can automatically create the next version number\n\nimport ast\n\ndef get_next_version(reg, model_name) -> str:\n    \"\"\"\n    Returns the next version of a model based on the existing versions in the registry.\n\n    Args:\n        reg: The registry object that provides access to the models.\n        model_name: The name of the model.\n\n    Returns:\n        str: The next version of the model in the format \"V_<version_number>\".\n\n    Raises:\n        ValueError: If the version list for the model is empty or if the version format is invalid.\n    \"\"\"\n    models = reg.show_models()\n    if models.empty:\n        return \"V_1\"\n    elif model_name not in models[\"name\"].to_list():\n        return \"V_1\"\n    max_version_number = max(\n        [\n            int(version.split(\"_\")[-1])\n            for version in ast.literal_eval(\n                models.loc[models[\"name\"] == model_name, \"versions\"].values[0]\n            )\n        ]\n    )\n    return f\"V_{max_version_number + 1}\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40b6b856-4545-40d9-b99d-0b6a93858329",
   "metadata": {
    "language": "python",
    "name": "Check_Distribution",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# check distribution to see how balanced out data set is\n# we will also filter out rows where the rank difference is 0, shouldnt be any...\n\ndf_training = session.table(f'final_data_{user_name}')\n\n# ignore games where there's no rank difference\ndf_training = df_training.filter( \n    (F.col('team_1_vs_team_2_rank') != 0) & \n    (F.col('team_1_vs_team_2_rank').is_not_null())\n) \n\ndf_training.group_by('game_outcome').agg(F.count('ID')).sort(F.col('game_outcome'))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9f54025-3b5b-4931-b37d-83467dce796c",
   "metadata": {
    "language": "sql",
    "name": "Scale_Up_Warehouse",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- we want to do some hyperparameter tuning, in order to speed things up lets size up our warehouse\n-- note - this is just temporary\n\nalter warehouse euro2024_wh set warehouse_size = xxlarge",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3543b6b-dfce-4ca6-ad05-cfe0e59eaa0c",
   "metadata": {
    "language": "python",
    "name": "Hyperparameter_Tuning",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# and now lets run Hyper Parameter tuning to get the best parameters\n# this should take about 1 min 20s, on a 2XL\n\n# hyper parameter grid is 6x6x7, with 5 folds thats 1,260 versions!\n\nfrom snowflake.ml.modeling.preprocessing import StandardScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.model_selection.grid_search_cv import GridSearchCV\n\ntrain_data = df_training\n\nFEATURE_COLS = [c for c in train_data.columns if c != \"GAME_OUTCOME\" and c != \"ID\"]\nLABEL_COLS = [\"GAME_OUTCOME\"]\n\nhyperparam_grid = {\n    \"n_estimators\": [50, 100, 200, 300, 400, 500],\n    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3, 0.4],\n    \"max_depth\": [3, 4, 5, 6, 7, 8, 9]\n}\n\npipeline = Pipeline(\n    steps = [\n        (\n            \"scaler\", \n            StandardScaler(\n                input_cols=FEATURE_COLS, \n                output_cols=FEATURE_COLS\n            )\n        ),\n        (\n        \"GridSearchCV\",\n            GridSearchCV(\n                estimator=XGBClassifier(random_state=42),\n                param_grid=hyperparam_grid,\n                scoring='accuracy', \n                label_cols=LABEL_COLS,\n                input_cols=FEATURE_COLS\n            )   \n        )\n    ]\n)\n\npipeline.fit(train_data)\n\nsklearn_hp = pipeline.to_sklearn()\noptimal_params = sklearn_hp.steps[-1][1].best_params_\nscore_dict = {\"best_accuracy\": sklearn_hp.steps[-1][1].best_score_}\n\nst.write(score_dict)\nst.write(optimal_params)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98ef2e7a-4688-426f-a3b1-5bcef44417d3",
   "metadata": {
    "language": "sql",
    "name": "Scale_Down_Warehouse",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- now we can scale it back down, it a matter of seconds\n\nalter warehouse euro2024_wh set warehouse_size = xsmall",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87bb3d6a-3d50-41f6-a7ea-3733660e89dc",
   "metadata": {
    "language": "python",
    "name": "Model_Training",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# taking our optimal parameters we're going to build our model\n\nfrom snowflake.ml.modeling.preprocessing import StandardScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.metrics import *\n\ntrain_data, test_data = df_training.random_split(weights=[0.8, 0.2], seed=0)\n\nFEATURE_COLS = [c for c in train_data.columns if c != \"GAME_OUTCOME\" and c != \"ID\"]\nLABEL_COLS = [\"GAME_OUTCOME\"]\n\npipeline = Pipeline(\n    steps = [\n        (\n            \"scaler\", \n            StandardScaler(\n                input_cols=FEATURE_COLS, \n                output_cols=FEATURE_COLS\n            )\n        ),\n        (\n            \"model\", \n            XGBClassifier(\n                input_cols=FEATURE_COLS, \n                label_cols=LABEL_COLS,\n                max_depth=optimal_params['max_depth'],\n                n_estimators = optimal_params['n_estimators'],\n                learning_rate = optimal_params['learning_rate']\n            )\n        )\n    ]\n)\n\npipeline.fit(train_data)\n\n# get the model accuracy\npredict_on_training_data = pipeline.predict(train_data)\ntraining_accuracy = accuracy_score(df=predict_on_training_data, y_true_col_names=[\"GAME_OUTCOME\"], y_pred_col_names=[\"OUTPUT_GAME_OUTCOME\"])\npredict_on_test_data = pipeline.predict(test_data)\neval_accuracy = accuracy_score(df=predict_on_test_data, y_true_col_names=[\"GAME_OUTCOME\"], y_pred_col_names=[\"OUTPUT_GAME_OUTCOME\"])\n\nst.write(f\"Training accuracy: {training_accuracy} \\nEval accuracy: {eval_accuracy}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac1dd149-42d1-48e5-a125-d312ff172787",
   "metadata": {
    "name": "_Model_Registry",
    "collapsed": false
   },
   "source": "## Model Registry\n---\n\n- Once the model is ready we'll use it to predict results of group stage.\n- Save the model using MLOps Model Registry features."
  },
  {
   "cell_type": "code",
   "id": "0c6956be-1db9-42e8-80cd-adc880c86783",
   "metadata": {
    "language": "python",
    "name": "Model_Registration",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\nreg = Registry(session=session)\n\nmodel_name = \"EURO_24_GAME_PREDICT\"\nmodel_version = get_next_version(reg, model_name)\n\nreg.log_model(\n    model_name=model_name,\n    version_name=model_version,\n    model=pipeline,\n    metrics={'training_accuracy':training_accuracy, 'eval_accuracy':eval_accuracy},\n    options={'relax_version': False}\n)\n\nm = reg.get_model(model_name)\nm.default = model_version",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f77d1126-ee25-452e-a0e6-fc4f57750b53",
   "metadata": {
    "language": "python",
    "name": "Model_Versioning",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# lets see the models we have in our registry\n\nreg.get_model(model_name).show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f57f506-5097-4129-984d-d78f806473b1",
   "metadata": {
    "name": "_Summary",
    "collapsed": false
   },
   "source": "# Summary\n\nWe now have a model in our registry we can use to call from either Snowpark or SQL, which we'll use in the predictions notebook"
  }
 ]
}